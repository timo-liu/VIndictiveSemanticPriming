\begin{table}[ht]
\centering
\small
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lccccccc}
\toprule
Component & Aunsiels/ChildBERT & FacebookAI/xlm-roberta-base & albert-base-v2 & bert-base-uncased & distilbert-base-uncased & microsoft/mpnet-base & roberta-base \\
\midrule
word\_embeddings & -0.016 & -0.010 & -0.013 & 0.008 & 0.014 & -0.032 & -0.053 \\
encoder\_layer\_1 & -0.048 & -0.035 &  & 0.000 & -0.028 & -0.005 & -0.071$^{\dagger}$ \\
encoder\_layer\_2 & -0.038 & -0.035 &  & 0.017 & -0.045 & -0.009 & -0.054 \\
encoder\_layer\_3 & -0.041 & -0.025 &  & 0.018 & -0.049 & -0.008 & -0.057 \\
encoder\_layer\_4 & -0.040 & -0.020 &  & 0.018 & -0.057 & -0.009 & -0.049 \\
encoder\_layer\_5 & -0.046 & -0.021 &  & 0.011 & -0.050 & -0.008 & -0.046 \\
encoder\_layer\_6 & -0.047 & -0.015 &  & 0.013 & -0.049 & -0.012 & -0.041 \\
encoder\_layer\_7 & -0.048 & -0.032 &  & 0.008 &  & -0.014 & -0.039 \\
encoder\_layer\_8 & -0.051 & -0.038 &  & -0.006 &  & 0.034 & -0.037 \\
encoder\_layer\_9 & -0.050 & -0.039 &  & -0.008 &  & -0.004 & -0.035 \\
encoder\_layer\_10 & -0.046 & -0.023 &  & -0.002 &  & -0.015 & -0.045 \\
encoder\_layer\_11 & -0.040 & -0.038 &  & -0.003 &  & -0.023 & -0.049 \\
encoder\_layer\_12 & -0.040 & -0.019 &  & 0.002 &  & 0.002 & -0.046 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Spearman $\rho$ between cosine similarity and RT (LDT, relation fpa, ISI=1050 ms). $^*$ $p<.01$, $^\dagger$ $.01\leq p \leq .05$.}
\label{tab:ldt_relfpa_isi1050}
\end{table}
