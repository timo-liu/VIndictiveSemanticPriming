\begin{table}[ht]
\centering
\small
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lccccccc}
\toprule
Component & Aunsiels/ChildBERT & FacebookAI/xlm-roberta-base & albert-base-v2 & bert-base-uncased & distilbert-base-uncased & microsoft/mpnet-base & roberta-base \\
\midrule
word\_embeddings & 0.134 & 0.045 & -0.033 & 0.170 & 0.209 & -0.008 & 0.143 \\
encoder\_layer\_1 & -0.105 & 0.171 &  & 0.033 & -0.029 & 0.096 & 0.123 \\
encoder\_layer\_2 & -0.209 & 0.234 &  & -0.069 & -0.120 & 0.101 & 0.203 \\
encoder\_layer\_3 & -0.074 & 0.216 &  & -0.130 & -0.094 & 0.098 & 0.211 \\
encoder\_layer\_4 & -0.076 & 0.249$^{\dagger}$ &  & -0.124 & -0.016 & 0.093 & 0.189 \\
encoder\_layer\_5 & -0.114 & 0.201 &  & -0.101 & -0.059 & 0.066 & 0.173 \\
encoder\_layer\_6 & -0.131 & 0.223 &  & -0.067 & 0.028 & 0.065 & 0.175 \\
encoder\_layer\_7 & -0.164 & 0.270$^{\dagger}$ &  & -0.018 &  & 0.035 & 0.191 \\
encoder\_layer\_8 & -0.188 & 0.250$^{\dagger}$ &  & -0.026 &  & 0.064 & 0.156 \\
encoder\_layer\_9 & -0.153 & 0.185 &  & -0.014 &  & -0.045 & 0.175 \\
encoder\_layer\_10 & -0.175 & 0.177 &  & 0.075 &  & 0.109 & 0.144 \\
encoder\_layer\_11 & -0.121 & 0.227 &  & 0.096 &  & -0.086 & 0.122 \\
encoder\_layer\_12 & -0.100 & 0.257$^{\dagger}$ &  & -0.066 &  & -0.079 & 0.142 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Spearman $\rho$ between cosine similarity and RT (LDT, relation 2.0, ISI=1050 ms). $^*$ $p<.01$, $^\dagger$ $.01\leq p \leq .05$.}
\label{tab:ldt_rel2.0_isi1050}
\end{table}
